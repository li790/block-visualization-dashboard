import streamlit as st
import pandas as pd
import hashlib
import time
from pathlib import Path
from typing import Dict, Any, Optional, Tuple
import pickle
import os

class CacheManager:
    """ç¼“å­˜ç®¡ç†å™¨ï¼Œç”¨äºç¼“å­˜å¤„ç†è¿‡çš„æ•°æ®ï¼Œå‡å°‘é‡å¤åŠ è½½æ—¶é—´"""
    
    def __init__(self, cache_dir: str = "output/buffer"):
        self.cache_dir = Path(cache_dir)
        self.cache_dir.mkdir(parents=True, exist_ok=True)
        self.cache_metadata_file = self.cache_dir / "cache_metadata.pkl"
        self.cache_metadata = self._load_cache_metadata()
        
    def _load_cache_metadata(self) -> Dict[str, Any]:
        """åŠ è½½ç¼“å­˜å…ƒæ•°æ®"""
        if self.cache_metadata_file.exists():
            try:
                with open(self.cache_metadata_file, 'rb') as f:
                    return pickle.load(f)
            except:
                return {}
        return {}
    
    def _save_cache_metadata(self):
        """ä¿å­˜ç¼“å­˜å…ƒæ•°æ®"""
        try:
            with open(self.cache_metadata_file, 'wb') as f:
                pickle.dump(self.cache_metadata, f)
        except Exception as e:
            st.warning(f"ä¿å­˜ç¼“å­˜å…ƒæ•°æ®å¤±è´¥: {e}")
    
    def _generate_cache_key(self, file_path: str, include_self_owned_labor: bool, 
                           file_size: int, file_mtime: float) -> str:
        """ç”Ÿæˆç¼“å­˜é”®"""
        # ä½¿ç”¨æ–‡ä»¶è·¯å¾„ã€å‚æ•°ã€å¤§å°å’Œä¿®æ”¹æ—¶é—´ç”Ÿæˆå”¯ä¸€é”®
        key_data = f"{file_path}_{include_self_owned_labor}_{file_size}_{file_mtime}"
        return hashlib.md5(key_data.encode()).hexdigest()
    
    def _get_file_info(self, file_path: Path) -> Tuple[int, float]:
        """è·å–æ–‡ä»¶ä¿¡æ¯"""
        try:
            stat = file_path.stat()
            return stat.st_size, stat.st_mtime
        except:
            return 0, 0
    
    def get_cached_data(self, file_path: str, include_self_owned_labor: bool) -> Optional[Tuple[pd.DataFrame, pd.DataFrame]]:
        """è·å–ç¼“å­˜çš„æ•°æ®"""
        try:
            file_path_obj = Path(file_path)
            if not file_path_obj.exists():
                return None
                
            file_size, file_mtime = self._get_file_info(file_path_obj)
            cache_key = self._generate_cache_key(str(file_path), include_self_owned_labor, file_size, file_mtime)
            
            # æ£€æŸ¥ç¼“å­˜æ˜¯å¦å­˜åœ¨ä¸”æœ‰æ•ˆ
            if cache_key in self.cache_metadata:
                cache_info = self.cache_metadata[cache_key]
                cache_file = self.cache_dir / f"{cache_key}.pkl"
                
                # æ£€æŸ¥ç¼“å­˜æ–‡ä»¶æ˜¯å¦å­˜åœ¨
                if cache_file.exists():
                    # æ£€æŸ¥ç¼“å­˜æ˜¯å¦è¿‡æœŸï¼ˆ24å°æ—¶ï¼‰
                    if time.time() - cache_info['timestamp'] < 86400:
                        try:
                            with open(cache_file, 'rb') as f:
                                cached_data = pickle.load(f)
                                return cached_data['main_df'], cached_data['tertiary_df']
                        except Exception as e:
                            # åˆ é™¤æ— æ•ˆç¼“å­˜
                            self._remove_cache(cache_key)
                    else:
                        self._remove_cache(cache_key)
            
            return None
            
        except Exception as e:
            return None
    
    def save_cached_data(self, file_path: str, include_self_owned_labor: bool, 
                        main_df: pd.DataFrame, tertiary_df: pd.DataFrame):
        """ä¿å­˜æ•°æ®åˆ°ç¼“å­˜"""
        try:
            file_path_obj = Path(file_path)
            if not file_path_obj.exists():
                return
                
            file_size, file_mtime = self._get_file_info(file_path_obj)
            cache_key = self._generate_cache_key(str(file_path), include_self_owned_labor, file_size, file_mtime)
            
            # ä¿å­˜æ•°æ®åˆ°ç¼“å­˜æ–‡ä»¶
            cache_file = self.cache_dir / f"{cache_key}.pkl"
            cache_data = {
                'main_df': main_df,
                'tertiary_df': tertiary_df,
                'file_path': str(file_path),
                'include_self_owned_labor': include_self_owned_labor
            }
            
            with open(cache_file, 'wb') as f:
                pickle.dump(cache_data, f)
            
            # æ›´æ–°ç¼“å­˜å…ƒæ•°æ®
            self.cache_metadata[cache_key] = {
                'file_path': str(file_path),
                'include_self_owned_labor': include_self_owned_labor,
                'file_size': file_size,
                'file_mtime': file_mtime,
                'timestamp': time.time(),
                'cache_file': str(cache_file)
            }
            
            self._save_cache_metadata()
            
        except Exception as e:
            pass
    
    def get_analysis_cache(self, cache_type: str, project_name: str, month: int, 
                          include_self_owned_labor: bool = False) -> Optional[Dict[str, Any]]:
        """è·å–åˆ†æç»“æœç¼“å­˜"""
        try:
            cache_key = f"{cache_type}_{project_name}_{month}_{include_self_owned_labor}"
            cache_file = self.cache_dir / f"analysis_{cache_key}.pkl"
            
            if cache_file.exists():
                # æ£€æŸ¥ç¼“å­˜æ˜¯å¦è¿‡æœŸï¼ˆ24å°æ—¶ï¼‰
                file_mtime = cache_file.stat().st_mtime
                if time.time() - file_mtime < 86400:
                    try:
                        with open(cache_file, 'rb') as f:
                            return pickle.load(f)
                    except Exception:
                        cache_file.unlink()
            
            return None
            
        except Exception:
            return None
    
    def save_analysis_cache(self, cache_type: str, project_name: str, month: int, 
                           data: Dict[str, Any], include_self_owned_labor: bool = False):
        """ä¿å­˜åˆ†æç»“æœç¼“å­˜"""
        try:
            cache_key = f"{cache_type}_{project_name}_{month}_{include_self_owned_labor}"
            cache_file = self.cache_dir / f"analysis_{cache_key}.pkl"
            
            with open(cache_file, 'wb') as f:
                pickle.dump(data, f)
                
        except Exception:
            pass
    
    def get_secondary_fee_cache(self, project_name: str, month: int, 
                               include_self_owned_labor: bool = False) -> Optional[pd.DataFrame]:
        """è·å–äºŒçº§è´¹é¡¹ç¼“å­˜"""
        return self.get_analysis_cache("secondary_fee", project_name, month, include_self_owned_labor)
    
    def save_secondary_fee_cache(self, project_name: str, month: int, 
                                data: pd.DataFrame, include_self_owned_labor: bool = False):
        """ä¿å­˜äºŒçº§è´¹é¡¹ç¼“å­˜"""
        self.save_analysis_cache("secondary_fee", project_name, month, data, include_self_owned_labor)
    
    def get_anomaly_cache(self, project_name: str, month: int, 
                         include_self_owned_labor: bool = False) -> Optional[Dict[str, Any]]:
        """è·å–å¼‚å¸¸æ•°æ®ç¼“å­˜"""
        return self.get_analysis_cache("anomaly", project_name, month, include_self_owned_labor)
    
    def save_anomaly_cache(self, project_name: str, month: int, 
                          data: Dict[str, Any], include_self_owned_labor: bool = False):
        """ä¿å­˜å¼‚å¸¸æ•°æ®ç¼“å­˜"""
        self.save_analysis_cache("anomaly", project_name, month, data, include_self_owned_labor)
    
    def get_project_analysis_cache(self, project_name: str, month: int, 
                                  include_self_owned_labor: bool = False) -> Optional[Dict[str, Any]]:
        """è·å–é¡¹ç›®è¯¦ç»†åˆ†æç¼“å­˜"""
        return self.get_analysis_cache("project_analysis", project_name, month, include_self_owned_labor)
    
    def save_project_analysis_cache(self, project_name: str, month: int, 
                                   data: Dict[str, Any], include_self_owned_labor: bool = False):
        """ä¿å­˜é¡¹ç›®è¯¦ç»†åˆ†æç¼“å­˜"""
        self.save_analysis_cache("project_analysis", project_name, month, data, include_self_owned_labor)
    
    def _remove_cache(self, cache_key: str):
        """åˆ é™¤æŒ‡å®šçš„ç¼“å­˜"""
        try:
            if cache_key in self.cache_metadata:
                cache_file = Path(self.cache_metadata[cache_key]['cache_file'])
                if cache_file.exists():
                    cache_file.unlink()
                del self.cache_metadata[cache_key]
                self._save_cache_metadata()
        except Exception as e:
            st.warning(f"åˆ é™¤ç¼“å­˜å¤±è´¥: {e}")
    
    def clear_all_cache(self):
        """æ¸…é™¤æ‰€æœ‰ç¼“å­˜"""
        try:
            # åˆ é™¤æ‰€æœ‰ç¼“å­˜æ–‡ä»¶
            for cache_file in self.cache_dir.glob("*.pkl"):
                if cache_file.name != "cache_metadata.pkl":
                    cache_file.unlink()
            
            # æ¸…ç©ºå…ƒæ•°æ®
            self.cache_metadata = {}
            self._save_cache_metadata()
            st.success("ğŸ—‘ï¸ å·²æ¸…é™¤æ‰€æœ‰ç¼“å­˜")
            
        except Exception as e:
            st.error(f"æ¸…é™¤ç¼“å­˜å¤±è´¥: {e}")
    
    def get_cache_stats(self) -> Dict[str, Any]:
        """è·å–ç¼“å­˜ç»Ÿè®¡ä¿¡æ¯"""
        try:
            cache_files = [f for f in self.cache_dir.glob("*.pkl") if f.name != "cache_metadata.pkl"]
            total_size = sum(f.stat().st_size for f in cache_files)
            
            return {
                'cache_count': len(cache_files),
                'total_size_mb': round(total_size / (1024 * 1024), 2),
                'metadata_count': len(self.cache_metadata)
            }
        except Exception as e:
            return {'error': str(e)}
    
    def cleanup_expired_cache(self):
        """æ¸…ç†è¿‡æœŸçš„ç¼“å­˜"""
        try:
            current_time = time.time()
            expired_keys = []
            
            for cache_key, cache_info in self.cache_metadata.items():
                if current_time - cache_info['timestamp'] > 86400:  # 24å°æ—¶
                    expired_keys.append(cache_key)
            
            for cache_key in expired_keys:
                self._remove_cache(cache_key)
            
            # é™é»˜æ¸…ç†è¿‡æœŸç¼“å­˜
                
        except Exception as e:
            st.warning(f"æ¸…ç†è¿‡æœŸç¼“å­˜å¤±è´¥: {e}")

# å…¨å±€ç¼“å­˜ç®¡ç†å™¨å®ä¾‹
cache_manager = CacheManager()

def get_cache_manager() -> CacheManager:
    """è·å–å…¨å±€ç¼“å­˜ç®¡ç†å™¨å®ä¾‹"""
    return cache_manager 